<!-- Generated by SpecStory -->

# Improve AI assistant movie matching (2025-07-15 17:27Z)

_**User**_

review this file and come up with suggestions on how to improve this undesirable behavior.
Search the internet, review the TMDB API, look up best practices
Review @chat-data-flow-state-report.md 
AI Assistant seems to consistently give movie + year which serves as a the basis for our first API call to TMDB if I understand correctly
This first search returns many results and we then try to add points to determine which is the best match given what AI recommended?

This seems a bit fragil, I wonder if there are other ways we could ensure a higher match rate between what the ai assistant is mentionning and what posters we're showing the user

Don't code or change any files. Just research deeply and make recommendations.

---

_**Assistant**_

<think><details><summary>Thought Process</summary>**Reviewing potential improvements**

The user is looking for a review of the file related to movie poster mismatches, specifically when the system selects the wrong poster for AI-recommended movies. They want to gather information from the internet, particularly regarding best practices with the TMDB API. Instead of code changes, I’ll focus on providing explanations for possible improvements to the algorithm and system. It seems like they'll also explore advanced search methods and parameters to enhance matching accuracy. I'll look into those options!
**Exploring improvement suggestions**

I'm considering searching for best practices related to fuzzy matching titles, like using Levenshtein distance or external metadata like IMDb ID. The user wants suggestions rather than code review, but I might still analyze some algorithm details to provide better context. Since I'll focus on proposing improvements, I'll start by searching for the "selectBestMatch" function to understand its workings, and I can refer to relevant TMDB API documentation for effective recommendations. Let's outline specific searches to gather actionable insights!
**Proposing API enhancements**

I'm considering alternative approaches for calling TMDB's movie search endpoint using the 'query' and 'year' parameters, while also exploring the inclusion of 'primary_release_year'. To improve fuzzy matching, I could utilize string similarity methods like Jaro-Winkler or Levenshtein, and account for nuances like M-dashes and synonyms. Another idea is to filter search results for exact titles and check release years if available while considering popularity. I should also look into storing title mappings in a database for future use and possibly implement caching. I'll gather relevant code for further analysis!</details></think>

---

<details>
            <summary>Grep search for "selectBestMatch" • **1** files</summary>
        
| File | Line | Match |
|------|------|-------|
| `file:///Users/guillaumecaronm4/Documents/dev/FRACTAL-BOOTCAMP/week2-chatbot/src/app/api/chat-tmdb-tool/route.ts` | L169 | `const selectBestMatch = (results: TMDBSearchResult[]): TMDBSearchResult | null => {` |
| `file:///Users/guillaumecaronm4/Documents/dev/FRACTAL-BOOTCAMP/week2-chatbot/src/app/api/chat-tmdb-tool/route.ts` | L228 | `const item = selectBestMatch(searchData.results);` |

</details>

---



---

Read file: src/app/api/chat-tmdb-tool/route.ts

---



---

<details>
            <summary>Searched web "TMDB API search/movie year parameter primary_release_year" • **6** results</summary>
        
| Title | URL |
|-------|-----|
| Movie | https://developer.themoviedb.org/reference/discover-movie |
| Search - Movies | https://developer.themoviedb.org/reference/search-movie |
| Release Dates | https://developer.themoviedb.org/reference/movie-release-dates |
| Search & Query For Details | https://developer.themoviedb.org/docs/search-and-query-for-details |
| Details | https://developer.themoviedb.org/reference/movie-details |
| Querying API by year or date range | https://www.themoviedb.org/talk/5ee7a74ecb71b8001eb6e9bd |

</details>

---



---

Below is a concentrated set of ideas — from quick-wins to larger-scope redesigns — that will significantly raise the hit-rate between the titles the AI mentions and the poster we display.  I drew from:

• TMDB’s official API docs (`search/movie`, `search/tv`, `discover/*`, `find/{external_id}`)  
• Several threads in TMDB’s support forums on disambiguation and scoring  
• A scan of your current `selectBestMatch` logic and chat-data flow documents  

──────────────────────────────────  
1. Make the SEARCH itself less ambiguous  
──────────────────────────────────  

1-A. Split the search by media-type first  
 • Instead of `/search/multi`, call `/search/movie` (and `/search/tv` as a fallback).  
 • Movies and TV results are separately ranked on TMDB’s side; removing people & collections lowers noise substantially.

1-B. Pass the **year filter** directly to TMDB  
 • Both `/search/movie` and `/search/tv` accept `year=` and `first_air_date_year=` (or `primary_release_year=` for discover).  
 • Supplying this parameter filters results before ranking, eliminating many near-name collisions.  
 • Implementation: extract the year as you already do, and append `&year=${searchYear}` (movies) or `&first_air_date_year=${searchYear}` (TV).

1-C. Use `region=` when you can infer locale  
 • TMDB sorts results differently by region.  
 • If the chat/session has locale info (or defaults to “US”), append `&region=US`; this bumps the local theatrical release to the top.

1-D. Try an **exact-phrase hint**  
 • TMDB supports `search_type=ngram` vs `phrase`.  
 • Adding `&search_type=phrase` after stripping punctuation forces the engine to prefer exact strings.

──────────────────────────────────  
2. Harden the SCORING algorithm  
──────────────────────────────────  

2-A. Aggressively reward an exact title match  
 • Raise the bonus from 50 → ~100 and apply an early-exit: if any result is an exact, same-year match, return it immediately.

2-B. Cap or remove popularity from the score  
 • Popularity works great for generic searches (“Batman”) but hurts specific picks.  Keep it only as a tie-breaker or scale it to ±3 points.

2-C. Introduce string-similarity distance  
 • Use Jaro-Winkler or fast-Levenshtein on lower-cased, de-punctuated titles.  
 • Award up to, say, 30 points for a similarity ≥ 0.95, tapering down to zero at 0.8.

2-D. Penalise mismatched media-types when the AI hints at one  
 • If the assistant’s phrase contains “series”, “show”, “season”, favour `tv` and subtract 20 if the candidate is a movie (and vice-versa).

──────────────────────────────────  
3. Enrich the TOOL interface to the AI  
──────────────────────────────────  

3-A. Add an optional `year` parameter to the `media_lookup` tool schema  
 • The policy already nudges the model to include it in text; exposing `year` explicitly removes string-parsing ambiguity.  
 • Example invocation: `media_lookup({ title: "The Thing", year: 1982 })`.

3-B. Let the model provide a TMDB or IMDb ID if it knows it  
 • Update the `execute` signature to accept `tmdbId` | `imdbId`.  
 • If supplied, skip search entirely and hit `/movie/{id}` or `/find/{imdb_id}`.

──────────────────────────────────  
4. Secondary SEARCH pass for tough cases  
──────────────────────────────────  

4-A. Title-only first, Discover API second  
 • If the primary search scores under a confidence threshold (e.g., < 80), run a second query through `/discover/movie?with_text_query=` or `/search/collection` to catch bundled series or alternate titles.

4-B. Alternative titles endpoint  
 • For the top 2–3 candidates, call `/movie/{id}/alternative_titles`.  
 • If any alt title matches the AI string exactly, select that item even if its initial score was lower.

──────────────────────────────────  
5. Feedback & Learning Loop  
──────────────────────────────────  

5-A. Persist the (title → TMDB id) pair once a poster is shown  
 • On every successful lookup, upsert a small mapping table.  
 • Subsequent identical titles skip searching, guaranteeing 100 % precision over time.

5-B. “User-correction” quick fix  
 • If the user clicks a wrong poster, present a lightweight search overlay so they can pick the correct one.  
 • Store that correction to improve future auto-selection.

──────────────────────────────────  
6. Prompt-engineering tweaks (cheap, high leverage)  
──────────────────────────────────  

• Strengthen the system prompt line:  
  “When you recommend a title, ALWAYS include its release year in parentheses – this is essential for accurate poster matching.”  
• Add: “If a title has multiple well-known versions, give both title and year explicitly.”  
• Instruct the model to avoid recommending anything it’s not sure about (“That new indie flick from last month”) unless it can supply the year.

──────────────────────────────────  
7. Optional advanced avenue  
──────────────────────────────────  

• Build a small in-memory Trie of **“very common ambiguous titles”** (e.g., “It”, “Carrie”, “King Kong”) and short-circuit to a stricter match routine when such a title is encountered.  
• Cache can live in the serverless edge for ultra-fast lookups.

──────────────────────────────────  
Rollout order I’d suggest  
──────────────────────────────────  
1. Switch to per-type endpoints & pass `year` (1-A + 1-B) – single-line change, biggest impact.  
2. Retune scoring (2-A..2-C) – minor code tweaks.  
3. Prompt update forcing year – zero code, immediate quality jump.  
4. Persist mapping & popularity cap – incremental DB & logic change.  
5. Expose `year` (and later `tmdbId`) in the tool schema – contracts update + AI retraining.  
6. Optional UX feedback / advanced matching.

By applying just the first three steps you should see the majority of mismatches disappear; the later items harden the system for edge cases and continual improvement.

---


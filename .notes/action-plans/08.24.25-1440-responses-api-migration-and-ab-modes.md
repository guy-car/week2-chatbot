# Action Plan: Responses API Migration + A/B Modes with plan_picks

## 0. Guiding Principles & Execution Protocol (NEW)
> **This preamble is the most critical part of the plan and must be followed strictly.**

1. **Immutable Plan**: Follow this plan exactly. If any step is impossible, stop and report immediately.
2. **Documentation is Law**: Use OpenAI official docs for the Responses API, Tools (Function Calling), Structured Outputs, `previous_response_id`, `reasoning.effort`, and `verbosity`. Do not rely on guesswork. `.docs/ressources` has OpenAI official documentation; reference it as often as necessary.
3. **One Step at a Time**: Perform a single action, then verify before continuing.
4. **Additive & Reversible**: Implement a new route (`/api/chat-v2`) and new codepaths. Keep the existing `/api/chat-tmdb-tool` (GPT‑4o + Vercel AI SDK) intact for rollback.
5. **Minimal Context Bloat**: Do not inject TMDB payloads into the model context. Keep inputs compact and purpose-built.
6. **Determinism & Guardrails**: Mode decision and pick planning are explicit tools; server performs lookups; enforce de-duplication server-side by TMDB ID with title/year as backup.
7. **Observability**: Add structured logs at each step (mode decision, planning inputs/outputs, lookups, dedupe, timings).

---

## 1. Overall Goal
- Migrate the chat flow to the OpenAI Responses API (using GPT‑5) with a deterministic two-tool sequence: `decide_mode` (A vs B) → `plan_picks` (rationale + up to 3 picks).
- Keep the current GPT‑4o route intact for immediate rollback.
- Reduce filler text and ensure recommendations are deduped against this chat and the user’s lists (watchlist + seen), with server-side TMDB lookups and enrichment.

Not in scope:
- UI redesign or modal visual changes.
- Moving enrichment implementation details (kept as-is, server-driven).
- Conversation summaries (skipped for now; add later if needed).

---

## 2. Regression Risks & Mitigation
- **Risk: Streaming breakage during migration** → Add `/api/chat-v2` separately; keep old route. Verify Stage 1 (Mode A-only) before enabling tools.
- **Risk: Tool call sequencing errors** → Enforce tool order in prompt and server logic (reject user-visible text before required tools complete in Mode B).
- **Risk: Duplicate recommendations** → Two-layer guard: plan-time exclusion + server ID enforcement.
- **Risk: Token overuse** → Keep inputs compact; omit TMDB payloads from model context; skip conversation summary initially.
- **Risk: Slow UX** → Instrument per-step timestamps, parallelize TMDB lookups, consider smaller model for `decide_mode` with fallback.
- **Risk: Duplicate assistant entries** → Upsert assistant message per user turn instead of inserting a new row; coalesce retries.

---

## 3. Structured Action & Verification Plan (IMPROVED STRUCTURE)

### Part 1: New Route & Responses API (Stage 1: Mode A Only)

- **Action 1.1 (DONE):** Create new route `src/app/api/chat-v2/route.ts` calling the OpenAI Responses API (`model: "gpt-5"`) via HTTPS fetch (no new SDK dependency). Stage 1 returns a single full text (no streaming). Mode A only; no tools yet.
- **Verification 1.1 (DONE):** Send a simple user prompt; confirm the assistant text arrives and is persisted. No tools defined yet; response returns JSON for manual testing (curl/Postman).

- **Action 1.2 (DONE):** Add logging (prefix `[FLOW]`) for request start/end, timing summary, and final message save.
- **Verification 1.2 (DONE):** Logs appear with timestamps for a sample request.

- **Action 1.3 (Threading) (DONE):** Add `lastResponseId` to `chats` and pass `previous_response_id` when present. Update `lastResponseId` after each successful Responses API call.
- **Verification 1.3 (DONE):** For a second request on the same chat, the route sends `previous_response_id` and updates `lastResponseId`.

### Part 2: Mode Decision Tool (Stage 2)

- **Action 2.1 (DONE):** Define `decide_mode` as a function tool (Responses API) using strict JSON output validated server-side with Zod:
  - `{ mode: 'A' | 'B', reason: string }` (max 160 chars)
  - Prompt rule: "Always call `decide_mode` first. If mode=A, respond conversationally and do not call other tools this turn. If mode=B, do not write any user-visible text before calling `plan_picks`."
- **Verification 2.1:** Simulate A-type prompts (opinion/context) → tool returns A; model responds with NL only. Simulate B-type prompts (explicit rec requests) → tool returns B; model does not emit text yet (since `plan_picks` not added).

- **Action 2.2 (DONE):** Add logs `[FLOW] decide_mode: { mode, reason }`.
- **Action 2.3 (Plannable):** Evaluate smaller model for `decide_mode` (e.g., `gpt-5-mini`) to reduce latency.
  - Keep the same strict JSON schema and Zod validation.
  - Add server-side fallback: if parse/validate fails or confidence drops, re-run `decide_mode` on `gpt-5`.
  - Toggle with env `DECIDE_MODE_MODEL`.
- **Verification 2.2 (DONE):** Confirm logs with correct modes across sample prompts.

### Part 3: Planning Tool + Server Lookups (Stage 3)

- **Action 3.1 (PENDING):** Define `plan_picks` tool with inputs (server-provided), using strict JSON validated with Zod:
  - `blocked`: `Array<{ id_tmdb: number; media_type:'movie'|'tv'; title: string; year: number }>` // union of watchlist + seen + already recommended
  - `tasteProfileSummary`: string (server-generated concise NL)
  - (Skip `conversationSummary` for now)
- **Verification 3.1:** The model returns:
  - `intro` (short string for the chat)
  - `picks`: 1–3 items `{ title, year, reason }` (reason = per-pick “why it fits”).

- **Action 3.2 (PENDING):** Prompt constraints: In Mode B, do not write any user-visible text before `plan_picks` completes. After it completes, emit the intro rationale only.
- **Verification 3.2:** Observe no generic preambles; the first text after B is the intro rationale.

- **Action 3.3 (PENDING):** Server-side TMDB lookups for each pick (search + best match), then enrichment as currently implemented. Do not send TMDB payloads to the model. If a pick fails verification/lookup/dedupe, drop it (no re-plan) and render remaining picks.
  - Performance: run TMDB lookups in parallel (Promise.all) to minimize turn latency.
- **Verification 3.3:** Each valid pick resolves to a TMDB ID; UI receives progressive data; no model context bloat; parallelization reduces lookup time vs sequential.

- **Action 3.5 (PENDING):** Run `tasteProfileSummary` generation and `blocked` retrieval in parallel.
  - Implementation: `Promise.all([getProfile+summary, listChatRecommendations+getUserBlockedList])`.
  - Verification 3.5: Lower pre-planning time vs sequential reads; no change in outputs.

- **Action 3.6 (PENDING):** Assistant message upsert per turn to avoid duplicate entries.
  - Design: One assistant message per user turn. Key by the last user message id (e.g., `asst_<userMsgId>`) and upsert to update intro/toolResults if the turn re-runs.
  - Threading: Keep `previous_response_id` threading for “same mind” continuity across turns.
  - Verification 3.6: Repeating the same request for the same `chatId` and user message updates the same assistant row (no duplicates), and recommendations still dedupe by `id_tmdb`.

- **Action 3.4 (PENDING):** Deduplication enforcement:
  - Plan-time: ensure `plan_picks` input includes `blocked` (current chat recommendations + user watchlist + seen)—instruct to avoid.
  - Enforcement-time: when resolving, reject IDs already in `blocked`; drop failed picks.
- **Verification 3.4:** Attempts to re-recommend an already suggested ID are blocked.

### Part 4: Observability & Safety

- **Action 4.1 (DONE for Stage 1–2):** Add console logs with timing for: `decide_mode`, (and later) `plan_picks` inputs/outputs (summarized), lookups per pick, dedupe events, overall turn summary. Prefer summaries; expand logs only when debugging.
  - UX Latency Note: monitor end-to-end response time; keep under a target (e.g., <4–6s). Track per-step durations and parallelization impact to avoid slow UX.
  - Timestamps: log per-step durations with consistent labels: `t_decide_mode`, `t_read_profile`, `t_read_blocked`, `t_plan_picks`, `t_tmdb_lookups_total`, `t_db_writes`, `t_total`.
- **Verification 4.1 (DONE for Stage 1–2):** Logs show expected sequence and durations for sample sessions.

- **Action 4.2:** Keep a per-chat recommended set server-side and append `{ id_tmdb, media_type, title, year }` after each successful lookup.
- **Verification 4.2:** Subsequent turns receive updated `exclude` from this set; duplicates are avoided.

### Part 5: Rollback & Toggle

- **Action 5.1 (DONE):** Add client env flag `NEXT_PUBLIC_CHAT_API_VERSION=v1|v2` to switch between `/api/chat-tmdb-tool` and `/api/chat-v2` in `src/app/_components/client/chat.tsx`.
- **Verification 5.1 (DONE):** When set to `v2` (post-streaming bridge), the client uses the new route; default remains `v1` until streaming is implemented.

---

## 4. Data Contracts (Final)

- `tasteProfileSummary`: server-generated concise NL (≤ 600 chars). Use existing generator.

- `blocked` (union of watchlist + seen + already recommended in this chat):
```ts
blocked: Array<{ id_tmdb: number; media_type: 'movie'|'tv'; title: string; year: number }>
```

- `exclude` (legacy alias within this doc) → replaced by `blocked`.

-- `plan_picks` output:
```ts
{
  intro: string,
  picks: Array<{ title: string; year: number; reason: string }>, // 1..3, reason = per-pick "why it fits"
}
```

---

## 5. Prompt Rules (Key Points)
- Always call `decide_mode` first.
- If A: respond conversationally; no tools this turn.
- If B: call `plan_picks` before any user-visible text. After it completes, write the intro rationale, then proceed with server lookups (no model tool calls).
- Picks: max 3, include year, avoid items in `blocked`.
- Keep responses concise; no generic preambles.

---

## 5.2. Mode B Response Contract (Server → Client)
- General intro: short string (assistant text).
- Per-pick planned details: `{ title, year, reason }` from `plan_picks`.
- Resolved items: TMDB-enriched `MovieData[]` for each accepted pick.
- Contract options (choose and implement consistently):
  1) Return both planned and resolved fields together per pick: `{ title, year, reason, resolved?: MovieData }`.
  2) Return `plannedPicks` and `resolvedPicks` arrays separately while preserving order.
- Rationale: reasons and explicit years are user-facing and should be available to the UI alongside resolved metadata.

---

## 5.1. New Chat Autostart Recommendations (Stage 3+)
- Desired behavior: when opening a new chat, seed an initial assistant message with a short greeting intro and 3 general recommendations.
- Inputs: use `tasteProfileSummary` and `blocked` (watchlist + seen + prior chat recs) to plan picks; no user prompt needed.
- Flow: call `plan_picks` immediately on chat creation; run TMDB lookups in parallel; persist assistant intro and resolved picks to messages/toolResults.
- Constraints: same dedupe and server enforcement rules; keep intro concise; avoid sending TMDB payloads to model.

---

## 6. Manual Checkpoints
- Stage 1 smoke: Simple conversational prompts work; logs present; no tools.
- Stage 2 smoke: `decide_mode` returns modes correctly; A emits text; B holds text (awaits planning).
- Stage 3 smoke: `plan_picks` returns valid picks + rationale; server resolves TMDB IDs; duplicates prevented.
- Performance smoke: parallel TMDB lookups reduce turn time compared to sequential; end-to-end latency acceptable for UX.
- Contract smoke: Mode B response includes general intro, per-pick reason, and year; UI renders all fields.

---

## 9. Model Strategy Notes (New)
- `decide_mode` can be latency-sensitive and is relatively simple classification under strict schema.
- Consider `gpt-5-mini` (or other low-latency model) for `decide_mode` with schema validation, then fall back to `gpt-5` on failure.
- Maintain consistent JSON schema across models; avoid prompt drift; keep `reasoning.effort: low` and `verbosity: low`.

---

## 6.1. Testing (Jest) — Write as We Implement
- Mode A flow: no tools; single NL message; no preambles. (DONE)
- Mode decision: `decide_mode` called first; A emits text; B emits no text pre-`plan_picks`. (DONE)
- Planning filters: excludes `exclude` and `userLists` items; returns ≤3 picks; schema compliance.
- Per-pick reasons: each pick has `title`, `year`, `reason`.
- Server enforcement: duplicates by `id_tmdb` rejected; failed picks are dropped (no re-plan).
- Route toggle/rollback: client switches between v1 and v2.

Live checks (Optional):
- Responses API live smoke (guarded): minimal prompt returns `output_text`. (DONE)

---

## 7. Rollback
- Flip client to legacy route; no code removal needed.
- Disable `/api/chat-v2` route if necessary.

---

## 8. References
- OpenAI Responses API: tools (function calling), streaming, structured outputs, `reasoning.effort`, `verbosity`, `previous_response_id`.

# Action Plan: Responses API Migration + A/B Modes with plan_picks

## 0. Guiding Principles & Execution Protocol (NEW)
> **This preamble is the most critical part of the plan and must be followed strictly.**

1. **Immutable Plan**: Follow this plan exactly. If any step is impossible, stop and report immediately.
2. **Documentation is Law**: Use OpenAI official docs for the Responses API, Tools (Function Calling), Structured Outputs, `previous_response_id`, `reasoning.effort`, and `verbosity`. Do not rely on guesswork. `.docs/ressources` has OpenAI official documentation; reference it as often as necessary.
3. **One Step at a Time**: Perform a single action, then verify before continuing.
4. **Additive & Reversible**: Implement a new route (`/api/chat-v2`) and new codepaths. Keep the existing `/api/chat-tmdb-tool` (GPT‑4o + Vercel AI SDK) intact for rollback.
5. **Minimal Context Bloat**: Do not inject TMDB payloads into the model context. Keep inputs compact and purpose-built.
6. **Determinism & Guardrails**: Mode decision and pick planning are explicit tools; server performs lookups; enforce de-duplication server-side by TMDB ID with title/year as backup.
7. **Observability**: Add structured logs at each step (mode decision, planning inputs/outputs, lookups, dedupe, timings).

---

## 1. Overall Goal
- Migrate the chat flow to the OpenAI Responses API (using GPT‑5) with a deterministic two-tool sequence: `decide_mode` (A vs B) → `plan_picks` (rationale + up to 3 picks).
- Keep the current GPT‑4o route intact for immediate rollback.
- Reduce filler text and ensure recommendations are deduped against this chat and the user’s lists (watchlist + seen), with server-side TMDB lookups and enrichment.

Not in scope:
- UI redesign or modal visual changes.
- Moving enrichment implementation details (kept as-is, server-driven).
- Conversation summaries (skipped for now; add later if needed).

---

## 2. Regression Risks & Mitigation
- **Risk: Streaming breakage during migration** → Add `/api/chat-v2` separately; keep old route. Verify Stage 1 (Mode A-only) before enabling tools.
- **Risk: Tool call sequencing errors** → Enforce tool order in prompt and server logic (reject user-visible text before required tools complete in Mode B).
- **Risk: Duplicate recommendations** → Two-layer guard: plan-time exclusion + server ID enforcement.
- **Risk: Token overuse** → Keep inputs compact; omit TMDB payloads from model context; skip conversation summary initially.

---

## 3. Structured Action & Verification Plan (IMPROVED STRUCTURE)

### Part 1: New Route & Responses API (Stage 1: Mode A Only)

- **Action 1.1:** Create new route `src/app/api/chat-v2/route.ts` calling the OpenAI Responses API (`model: "gpt-5"`) via HTTPS fetch (no new SDK dependency). Stage 1 returns a single full text (no streaming). Mode A only; no tools yet.
- **Verification 1.1:** Send a simple user prompt; confirm the assistant text arrives and is persisted. No tools defined yet; response returns JSON for manual testing (curl/Postman).

- **Action 1.2:** Add logging (prefix `[FLOW]`) for request start/end, timing summary, and final message save.
- **Verification 1.2:** Logs appear with timestamps for a sample request.

- **Action 1.3 (Threading):** Add `lastResponseId` to `chats` and pass `previous_response_id` when present. Update `lastResponseId` after each successful Responses API call.
- **Verification 1.3:** For a second request on the same chat, the route sends `previous_response_id` and updates `lastResponseId`.

### Part 2: Mode Decision Tool (Stage 2)

- **Action 2.1:** Define `decide_mode` as a function tool (Responses API) using strict JSON output validated server-side with Zod:
  - `{ mode: 'A' | 'B', reason: string }` (max 160 chars)
  - Prompt rule: "Always call `decide_mode` first. If mode=A, respond conversationally and do not call other tools this turn. If mode=B, do not write any user-visible text before calling `plan_picks`."
- **Verification 2.1:** Simulate A-type prompts (opinion/context) → tool returns A; model responds with NL only. Simulate B-type prompts (explicit rec requests) → tool returns B; model does not emit text yet (since `plan_picks` not added).

- **Action 2.2:** Add logs `[FLOW] decide_mode: { mode, reason }`.
- **Verification 2.2:** Confirm logs with correct modes across sample prompts.

### Part 3: Planning Tool + Server Lookups (Stage 3)

- **Action 3.1:** Define `plan_picks` tool with inputs (server-provided), using strict JSON validated with Zod:
  - `blocked`: `Array<{ id_tmdb: number; media_type:'movie'|'tv'; title: string; year: number }>` // union of watchlist + seen + already recommended
  - `tasteProfileSummary`: string (server-generated concise NL)
  - (Skip `conversationSummary` for now)
- **Verification 3.1:** The model returns:
  - `intro` (short string for the chat)
  - `picks`: 1–3 items `{ title, year, reason }` (reason = per-pick “why it fits”).

- **Action 3.2:** Prompt constraints: In Mode B, do not write any user-visible text before `plan_picks` completes. After it completes, emit the intro rationale only.
- **Verification 3.2:** Observe no generic preambles; the first text after B is the intro rationale.

- **Action 3.3:** Server-side TMDB lookups for each pick (search + best match), then enrichment as currently implemented. Do not send TMDB payloads to the model. If a pick fails verification/lookup/dedupe, drop it (no re-plan) and render remaining picks.
- **Verification 3.3:** Each valid pick resolves to a TMDB ID; UI receives progressive data; no model context bloat.

- **Action 3.4:** Deduplication enforcement:
  - Plan-time: ensure `plan_picks` input includes `blocked` (current chat recommendations + user watchlist + seen)—instruct to avoid.
  - Enforcement-time: when resolving, reject IDs already in `blocked`; drop failed picks.
- **Verification 3.4:** Attempts to re-recommend an already suggested ID are blocked.

### Part 4: Observability & Safety

- **Action 4.1:** Add console logs with timing for: `decide_mode`, `plan_picks` inputs/outputs (summarized), lookups per pick, dedupe events, overall turn summary. Prefer summaries; expand logs only when debugging.
- **Verification 4.1:** Logs show expected sequence and durations for sample sessions.

- **Action 4.2:** Keep a per-chat recommended set server-side and append `{ id_tmdb, media_type, title, year }` after each successful lookup.
- **Verification 4.2:** Subsequent turns receive updated `exclude` from this set; duplicates are avoided.

### Part 5: Rollback & Toggle

- **Action 5.1:** Add client env flag `NEXT_PUBLIC_CHAT_API_VERSION=v1|v2` to switch between `/api/chat-tmdb-tool` and `/api/chat-v2` in `src/app/_components/client/chat.tsx`.
- **Verification 5.1:** When set to `v2` (post-streaming bridge), the client uses the new route; default remains `v1` until streaming is implemented.

---

## 4. Data Contracts (Final)

- `tasteProfileSummary`: server-generated concise NL (≤ 600 chars). Use existing generator.

- `blocked` (union of watchlist + seen + already recommended in this chat):
```ts
blocked: Array<{ id_tmdb: number; media_type: 'movie'|'tv'; title: string; year: number }>
```

- `exclude` (legacy alias within this doc) → replaced by `blocked`.

-- `plan_picks` output:
```ts
{
  intro: string,
  picks: Array<{ title: string; year: number; reason: string }>, // 1..3, reason = per-pick "why it fits"
}
```

---

## 5. Prompt Rules (Key Points)
- Always call `decide_mode` first.
- If A: respond conversationally; no tools this turn.
- If B: call `plan_picks` before any user-visible text. After it completes, write the intro rationale, then proceed with server lookups (no model tool calls).
- Picks: max 3, include year, avoid items in `blocked`.
- Keep responses concise; no generic preambles.

---

## 6. Manual Checkpoints
- Stage 1 smoke: Simple conversational prompts work; logs present; no tools.
- Stage 2 smoke: `decide_mode` returns modes correctly; A emits text; B holds text (awaits planning).
- Stage 3 smoke: `plan_picks` returns valid picks + rationale; server resolves TMDB IDs; duplicates prevented.

---

## 6.1. Testing (Jest) — Write as We Implement
- Mode A flow: no tools; single NL message; no preambles.
- Mode decision: `decide_mode` called first; A emits text; B emits no text pre-`plan_picks`.
- Planning filters: excludes `exclude` and `userLists` items; returns ≤3 picks; schema compliance.
- Per-pick reasons: each pick has `title`, `year`, `reason`.
- Server enforcement: duplicates by `id_tmdb` rejected; failed picks are dropped (no re-plan).
- Route toggle/rollback: client switches between v1 and v2.

---

## 7. Rollback
- Flip client to legacy route; no code removal needed.
- Disable `/api/chat-v2` route if necessary.

---

## 8. References
- OpenAI Responses API: tools (function calling), streaming, structured outputs, `reasoning.effort`, `verbosity`, `previous_response_id`.
